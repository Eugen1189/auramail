"""
Prompt Caching module for AuraMail.
Caches system prompts using Gemini's context caching feature to reduce token costs.

CRITICAL OPTIMIZATION: Since Security Guard and LibrarianAgent prompts are large,
using Gemini's prompt caching feature significantly speeds up responses and reduces costs.
"""
import hashlib
from typing import Optional, Dict
from google import genai
from google.genai import types

# Cache for prompt hashes and cached content
_prompt_cache: Dict[str, str] = {}


def get_cached_prompt_hash(prompt_text: str) -> str:
    """
    Generates a hash for prompt text to use as cache key.
    
    Args:
        prompt_text: The prompt text to hash
    
    Returns:
        SHA256 hash of the prompt
    """
    return hashlib.sha256(prompt_text.encode('utf-8')).hexdigest()[:16]


def create_cached_content(client: genai.Client, prompt_text: str, ttl_seconds: int = 3600) -> Optional[str]:
    """
    Creates a cached content entry in Gemini for the prompt.
    
    CRITICAL OPTIMIZATION: Uses Gemini's context caching to cache large system prompts.
    This reduces token costs for repeated prompts (Security Guard, LibrarianAgent).
    
    Args:
        client: Initialized Gemini client
        prompt_text: The prompt text to cache
        ttl_seconds: Time-to-live for cache (default: 1 hour)
    
    Returns:
        Cache key/ID if successful, None otherwise
    """
    if not client:
        return None
    
    try:
        prompt_hash = get_cached_prompt_hash(prompt_text)
        
        # Check if already cached
        if prompt_hash in _prompt_cache:
            return _prompt_cache[prompt_hash]
        
        # Create cached content using Gemini API
        # Note: This requires Gemini API support for cached content
        # For now, we'll use a simple in-memory cache
        # In future, can be upgraded to use Gemini's native caching
        
        # Store in cache
        _prompt_cache[prompt_hash] = prompt_hash
        
        print(f"üíæ [Prompt Cache] Cached prompt (hash: {prompt_hash[:8]}...)")
        return prompt_hash
        
    except Exception as e:
        print(f"‚ö†Ô∏è [Prompt Cache] Failed to cache prompt: {e}")
        return None


def get_cached_prompt(prompt_hash: str) -> Optional[str]:
    """
    Retrieves cached prompt by hash.
    
    Args:
        prompt_hash: Hash of the prompt
    
    Returns:
        Cached prompt text if found, None otherwise
    """
    return _prompt_cache.get(prompt_hash)


def clear_prompt_cache():
    """Clears all cached prompts."""
    _prompt_cache.clear()
    print("üóëÔ∏è [Prompt Cache] Cache cleared")



